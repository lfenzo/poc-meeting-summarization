{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1af66a-cb93-482f-bf9a-485dce777829",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "import whisperx\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "HF_TOKEN = os.environ['HF_TOKEN']\n",
    "\n",
    "device = \"cuda\"\n",
    "audio_file = \"audios/video5.mp3\"\n",
    "audio_name = re.split('/|\\.', audio_file)[1]  # audios/video3.mp3 -> video3\n",
    "batch_size = 8  # reduce if low on GPU mem\n",
    "dialogues_dir = 'dialogues'\n",
    "diarized_outputs_dir = 'diarized_outputs'\n",
    "\n",
    "for directory in [dialogues_dir, diarized_outputs_dir]:\n",
    "    if not os.path.exists(os.path.join(directory)):\n",
    "        os.mkdir(os.path.join(directory))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93078038-c10a-4e1b-941e-0f42cc903001",
   "metadata": {},
   "source": [
    "## Transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b519657-9a20-48aa-a961-62256f652a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = whisperx.load_model(\"large-v2\", device, compute_type=\"float32\")  # change to \"int8\" if low on GPU mem (may reduce accuracy)\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "transcription = model.transcribe(audio, batch_size=batch_size)\n",
    "\n",
    "import gc; gc.collect(); torch.cuda.empty_cache(); del model\n",
    "\n",
    "transcription[\"segments\"] # before alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0e4352-63a3-4c2a-96d7-c36dcfee4cfa",
   "metadata": {},
   "source": [
    "## Output Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7b666-b6ba-4ff3-adda-487120720177",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_a, metadata = whisperx.load_align_model(language_code=transcription[\"language\"], device=device)\n",
    "alignment = whisperx.align(\n",
    "    transcription[\"segments\"],\n",
    "    model_a,\n",
    "    metadata,\n",
    "    audio,\n",
    "    device,\n",
    "    return_char_alignments=False\n",
    ")\n",
    "\n",
    "import gc; gc.collect(); torch.cuda.empty_cache(); del model_a\n",
    "\n",
    "alignment[\"segments\"]  # after alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3196560-50f1-4260-a80d-9ddd323310c6",
   "metadata": {},
   "source": [
    "## Assigning Speaker Labels (Diarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9d040f-0b38-4076-9c43-604e0ec21e5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "diarize_model = whisperx.DiarizationPipeline(use_auth_token=HF_TOKEN, device=device)\n",
    "\n",
    "# add min/max number of speakers if known a priori\n",
    "# diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n",
    "audio = whisperx.load_audio(audio_file)\n",
    "diarize_segments = diarize_model(audio)\n",
    "\n",
    "result = whisperx.assign_word_speakers(diarize_segments, alignment)\n",
    "import gc; gc.collect(); torch.cuda.empty_cache(); del diarize_model\n",
    "\n",
    "\n",
    "with open(os.path.join('.', 'diarized_outputs', f'{audio_name}_segments.pkl'), 'wb') as file:\n",
    "    pickle.dump((diarize_segments, result), file)\n",
    "    \n",
    "print(diarize_segments)\n",
    "print(result[\"segments\"])  # segments are now assigned speaker IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936b31c6-4ace-49a5-8679-c8e4c0aea139",
   "metadata": {},
   "source": [
    "## Processing Diarization Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2afd094-783f-4821-8430-90af1a812d46",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result['segments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46574d51-6772-47b0-9f24-abc83d1e450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dialogue_from_segments_fifo(segments: list[dict], speakers: dict = None) -> str:\n",
    "    \"\"\"Assign labels based on token-wise labeling performed by WhisperX.\"\"\"\n",
    "    dialogue = []\n",
    "    buffer = '' \n",
    "    current_speaker = None\n",
    "    \n",
    "    for segment in segments:\n",
    "        for word in segment['words']:\n",
    "            if not current_speaker:\n",
    "                current_speaker = word['speaker']\n",
    "            if 'speaker' not in word.keys() or current_speaker == word['speaker']:\n",
    "                buffer += f\" {word['word']}\"\n",
    "            else:\n",
    "                dialogue.append(f\"{current_speaker if not speakers else speakers[current_speaker]}: {buffer}\")\n",
    "                buffer = word['word']\n",
    "                current_speaker = word['speaker']\n",
    "    return '\\n'.join(dialogue)\n",
    "\n",
    "\n",
    "def generate_dialogue_from_segments_most_frequent(segments: list[dict], speakers: dict = None) -> str:\n",
    "    \"\"\"Assign the label to the most frequent speaker within the segment.\"\"\"\n",
    "    dialogue = []\n",
    "    for segment in segments:\n",
    "        df = pd.DataFrame().from_records(segment['words'])\n",
    "        most_frequent_speaker = df['speaker'].value_counts().to_frame().reset_index().loc[0, 'speaker']\n",
    "        dialogue.append(f\"{most_frequent_speaker if not speakers else speakers[most_frequent_speaker]}: {segment['text']}\")\n",
    "    return '\\n'.join(dialogue)\n",
    "\n",
    "\n",
    "def generate_dialogue_from_segments(segments: list[dict], speakers: dict = None) -> str:\n",
    "    \"\"\"Use WhisperX-assigned whole segment labels.\"\"\"\n",
    "    dialogue = []\n",
    "    for segment in segments:\n",
    "        dialogue.append(f\"{segment['speaker'] if not speakers else speakers[segment['speaker']]}: {segment['text']}\")\n",
    "    return '\\n'.join(dialogue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c5650c-ff8b-402a-809e-a65fd49597f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue = generate_dialogue_from_segments(\n",
    "    segments=result['segments'],\n",
    ")\n",
    "\n",
    "with open(os.path.join('.', 'dialogues', f'{audio_name}_dialogue.txt'), 'w') as file:\n",
    "    file.write(dialogue)\n",
    "\n",
    "print(dialogue)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
